
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PAg-NeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://claussmitt.com/pagnerf/assets/images/pagnerf.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://claussmitt.com/pagnerf/"/>
    <meta property="og:title" content="PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics" />
    <meta property="og:description" content="PAg-NeRF uses state-of-the-art accelerated NeRFs and online pose optimization to produce 3D consistent panoptic representations of challenging agricultural environments." />

    <link rel="shortcut icon" href="assets/images/peppers.ico" type="image/x-icon">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="/assets/css/styles.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>PAg-NeRF</b>: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics 
                <small>
                <!-- ICCV 2023 (Oral Presentation) -->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="http://claussmitt.com">
                        Claus Smitt ðŸŒ±</a>
                    </li>
                    <li>
                        <a href="http://agrobotics.uni-bonn.de/michael-halstead/">
                        Michael Halstead ðŸŒ±</a>
                    </li>
                    <li>
                        <a href="http://agrobotics.uni-bonn.de/patrick-zimmer/">
                        Patrick Zimmer ðŸŒ±</a>
                    </li>
                    <li>
                        <a href="https://www.ipb.uni-bonn.de/people/thomas-laebe/">
                        Thomas LÃ¤be ðŸ“¸</a>
                    </li>
                    <li>
                        <a href="http://agrobotics.uni-bonn.de/esra-guclu/">
                        Esra Guclu ðŸŒ±</a>
                    </li>
                    <li>
                        <a href="https://www.ipb.uni-bonn.de/people/cyrill-stachniss/">
                        Cyrill Stachniss ðŸ“¸</a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/christophersmccool/">
                        Chris McCool ðŸŒ±</a>
                    </li>
                    <br><br>
                    <a href="http://agrobotics.uni-bonn.de/">
                        ðŸŒ± AgRobotics, Institute of Agriculture</a> &nbsp;&nbsp;&nbsp;&nbsp 
                    <a href="https://www.ipb.uni-bonn.de/">
                        ðŸ“¸ Institute of Photogrammetry</a>
                    </br><a href="https://www.uni-bonn.de/">
                        University of Bonn</a>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2309.05339">
                            <image src="assets/images/paper_cover.png" height="60px">
                                <h4><strong>Pre-Print</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Agricultural-Robotics-Bonn/pagnerf">
                            <image src="assets/images/github.svg" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="http://agrobotics.uni-bonn.de/sweet_pepper_dataset/">
                            <image src="assets/images/database.svg" height="60px">
                                <h4><strong>Data</strong><br></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
        <br>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify">
                    <strong>TL;DR: </strong> PAg-NeRF is a fast and efficient model, targeted for challenging agricultural scenarios, that can render photo-realistic panoptic 3D maps from images, panoptic detections and robot odometry.
                </p>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="assets/videos/grid.mp4" type="video/mp4" />
                </video>
						</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Precise scene understanding is key for most robot monitoring and intervention tasks in agriculture.
                    In this work we present PAg-NeRF which is a novel NeRF-based system that enables 3D panoptic scene understanding.
                    Our representation is trained using an image sequence with noisy robot odometry poses and automatic panoptic predictions with inconsistent IDs between frames.
                    Despite this noisy input, our system is able to output scene geometry, photo-realistic renders and 3D consistent panoptic representations with consistent instance IDs.
                    We evaluate this novel system in a very challenging horticultural scenario and in doing so demonstrate an end-to-end trainable system that can make use of noisy robot poses rather than precise poses that have to be pre-calculated.
                    Compared to a baseline approach the peak signal to noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be tuned to improve inference time by more than a factor of 2 while being memory efficient with approximately 12 times fewer parameters.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                        <video id="v0" width="100%" controls>
                            <source src="assets/videos/paper_video.mp4" type="video/mp4" />
                        </video>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Architecture
                </h3>
                <img src='assets/images/pipeline.png' width="100%">
                
                <p class="text-justify">
                <br>
                We use state-of-the-art permutohedral feature hash-grids to encode 3D space, allowing our system to be fast and memory efficient. 
                
                Our architecture uses novel delta grid that computes panoptic features by correcting the color features, leveraging the similarity between modalities.

                Thanks to the implicit sparseness of hash-grids, we are able to reduce the panoptic capacity to only have valid values where corrections are needed.
                We avoid propagating gradients from the panoptic to the color branch to ensure the panoptic grid only learns corrections over the color features.
                
                Our grid based architecture allows us to decoded all render quantities with very shallow MLPs.

                To learn 3D consistent instant IDs from inconsistent ID still-image predictions, we employ a modified linear assignment loss, tackling the fix scale nature of several agricultural datasets, rejecting repeated IDs.

                To obtain high-detail multi-view consistent renders, we also perform online pose optimization, making our system end-to-end trainable.
                </p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-12">
                <h3>
                    Results Comparison <small>(click to enlarge)</small>
                </h3>
            </div>
            <div class="col-md-4">
                <video class="video" width=0 id="panoli_comp1" loop playsinline autoplay muted src="assets/videos/panoli_comp_3.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="panoli_comp1Merge" style="cursor: pointer;"></canvas>
                <p class="text-justify" id="panoli_comp1Div"></p>
            </div>

            <div class="col-md-4">
                <video class="video" width=0 id="panoli_comp2" loop playsinline autoplay muted src="assets/videos/panoli_comp_4.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="panoli_comp2Merge" style="cursor: pointer;"></canvas>
                <p class="text-justify" id="panoli_comp2Div"></p>
            </div>

            <div class="col-md-4">
                <video class="video" width=0 id="panoli_comp3" loop playsinline autoplay muted src="assets/videos/panoli_comp_1.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="panoli_comp3Merge" style="cursor: pointer;"></canvas>
                <p class="text-justify" id="panoli_comp3Div"></p>
            </div>
            <br>
            <div class="col-md-12">
                <p class="text-justify">
                    
                    PAg-NeRF is able to reproduce very fine details of the fruit and leaf textures as well as high frequency edges and thin structures that get smoothed out by Panoptic Lifting.
                    Most instances produced by PAg-NeRF are well segmented even though high levels of occlusion, varying illumination conditions and camera shake.
                    Additionally, thanks to our repeated ID rejection loss, our model is able to properly distinguish between instances at the far ends of the frame, whereas Panoptic Lifting merges them into a single detection with long bounding boxes since it uses a plain linear assignment loss during training.
                    PAg-NeRF achieves these results with very shallow NN decoders in contrast to panoptic lifting and can be tuned to be 2x Faster with 12x less parameters.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h3>
                    Key Model Advantages <small>(click to enlarge)</small>
                </h3>
            </div>
            <div class="col-md-4">
                <h4 style="margin-bottom:-3mm;">
                    Consistent Instance IDs
                </h4>
                <video class="video" width=0 id="inst_comp" loop autoplay muted src="assets/videos/inst_comp.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="inst_compMerge" style="cursor: pointer;"></canvas>
                <p class="text-justify" id="inst_compDiv">
                    Our model takes as input still-image panoptic detections from a Mask2Former, with instance IDs changing every frame, and extracts instances with consistent ID using its implicit 3D representation and a robust linear assignment loss.
                </p>
            </div>
            <div class="col-md-4">
                <h4 style="margin-bottom:-3mm;">
                    Repeated ID Rejection
                </h4>
                <video class="video" width=0 id="idrej_comp" loop playsinline autoplay muted src="assets/videos/idrej_comp.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="idrej_compMerge" style="cursor: pointer;"></canvas>
                <p class="text-justify" id="idrej_compDiv">
                    We proposed a modified linear assignment loss, robust to repeated ID assignment, for data with planar camera motion parallel to a quasi-planar distribution of targets.
                </p>
            </div>
            <div class="col-md-4">
                <h4 style="margin-bottom:-3mm;">
                    Online Pose Optimization
                </h4>
                <video class="video" width=0 id="odom_comp" loop playsinline autoplay muted src="assets/videos/odom_comp.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="odom_compMerge" style="cursor: pointer;"></canvas>
                <p class="text-justify" id="odom_compDiv">
                    Our model refines noisy robot odometry with online pose optimization to estimate the correct camera motion. This leads to both higher render and panoptic performance. 
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    3D Panoptic Maps
                </h3>
                <div class="text-center">
                        <video id="v0" width="100%" autoplay loop muted controls>
                            <source src="assets/videos/3dview.mp4" type="video/mp4" />
                        </video>
                </div>
                <p class="text-justify">
                    <br>
                    3D panoptic pointclouds can be extracted from our implicit representation, by rendering color and panoptic images and un-pojecting them with the corresponding estimated ray termination.
                    Individual fruit instances can be extracted from the panoptic map by keeping only points that have valid instance IDs.
                    This type of output can be used to count individual peppers or combine it with prior knowledge-based prediction approaches to regress a mesh representation of individual peppers, from which volume and quality can be inferred.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{smitt2023pag,
    title={PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D
           representations for agricultural robotics},
    author={Smitt Claus and Halstead Michael and Zimmer Patrick and
            Laebe Thomas and Guclu Esra and Stachniss Cyrill and McCool Chris},
    journal={arXiv preprint arXiv:2309.05339},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work was developed in collaboration with the <a href="http://aid4crops.uni-bonn.de/">Aid4Crops</a> project and the <a href="http://phenorob.de/">PhenoRob</a> Cluster of Excelence.
                    <br>
                    Further more this project was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) 459376902 and under Germanyâ€™s Excellence Strategy - EXC 2070 â€“ 390732324.
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
